{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/panditrahulsharma/Blood-Cell-Subtypes-Classification-Using-deep-learning-and-CNN-model/blob/master/prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "04n9CjCfe1nw",
    "outputId": "a62e0ecb-d622-4cf0-e60e-d3d16cb01f2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/content/drive/My Drive/blood_cell/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8NVeV-sUQDD"
   },
   "outputs": [],
   "source": [
    "# Pickle in Python is primarily used in serializing and deserializing a Python object structure. In other words, \n",
    "# it's the process of converting a Python object into a byte stream to store it in a file/database, maintain program\\\n",
    "# state across sessions, or transport data over the network.\n",
    "#To open the file for writing, simply use the open() function. The first argument should be the name of your file. The \n",
    "# second argument is 'wb'. The w means that you'll be writing to the file, and b refers to binary mode.\n",
    "#Once the file is opened for writing, you can use pickle.dump(), which takes two arguments: the object you want \n",
    "#to pickle and the file to which the object has to be saved. In this case, the former will be classifier, while \n",
    "# the latter will be model_pkl.\n",
    "# Close the pickle instances using pickle.close()\n",
    "\n",
    "\n",
    "import pickle\n",
    "digit_detect_pkl=open('Blood_cell.pkl','rb') #opening our pickle file of model in \"rb\" i.e. \"rb\" mode opens the file in binary format for reading.\n",
    "model = pickle.load(digit_detect_pkl) #loading our pickle object using .load method of pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTY9jsI7gqAc"
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow.compat.v1 as tf\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSesm-IWgrM5"
   },
   "outputs": [],
   "source": [
    "in_dir='/content/drive/My Drive/blood_cell/dataset2-master/dataset2-master/images/'\n",
    "train_dataset_path=in_dir+\"TRAIN/\"\n",
    "test_dataset_path=in_dir+\"TEST/\"\n",
    "CATEGORIES    = ['EOSINOPHIL','LYMPHOCYTE','MONOCYTE','NEUTROPHIL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "73dMGoeBgvhU",
    "outputId": "f0a40551-c8a1-4623-94f1-b15d3f06c958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2487 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(test_dataset_path,\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode = 'categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKMQZL_ggJdN"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict_generator(test_set) #making new prediction on our test_set using .predict_generator method from Keras ImageDataGenerator \n",
    "y_pred=y_pred.argmax(axis=-1) #argmax returns the index of maximum value whereas axix=-1 signifies that \n",
    "                              #that the index that will be returned by argmax will be taken from the last axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OV7nkzC41h1u"
   },
   "outputs": [],
   "source": [
    "y_true = test_set.classes #using .classes method of test_set to get all the actual classes indices for comparing \n",
    "                          #with predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qwijlIqi1lSV",
    "outputId": "c8c2d730-a606-4bb1-eea1-193dd9ae875c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION METRICS :\n",
    "\n",
    "\n",
    "# Classification Accuracy\n",
    "Classification Accuracy is what we usually mean, when we use the term accuracy. It is the ratio of number of correct predictions to the total number of input samples.\n",
    "![](https://miro.medium.com/max/373/1*yRa2inzTnyASJOre93ep3g.gif)\n",
    "\n",
    "It works well only if there are equal number of samples belonging to each class.\n",
    "For example, consider that there are 98% samples of class A and 2% samples of class B in our training set. Then our model can easily get 98% training accuracy by simply predicting every training sample belonging to class A.\n",
    "When the same model is tested on a test set with 60% samples of class A and 40% samples of class B, then the test accuracy would drop down to 60%. Classification Accuracy is great, but gives us the false sense of achieving high accuracy.\n",
    "The real problem arises, when the cost of misclassification of the minor class samples are very high. If we deal with a rare but fatal disease, the cost of failing to diagnose the disease of a sick person is much higher than the cost of sending a healthy person to more tests.\n",
    "\n",
    "# Logarithmic Loss\n",
    "Logarithmic Loss or Log Loss, works by penalising the false classifications. It works well for multi-class classification. When working with Log Loss, the classifier must assign probability to each class for all the samples. Suppose, there are N samples belonging to M classes, then the Log Loss is calculated as below :\n",
    "![](https://miro.medium.com/max/344/1*dtpzlB_dNrmxDq7fGnJ4cg.gif)\n",
    "where,\n",
    "y_ij, indicates whether sample i belongs to class j or not\n",
    "p_ij, indicates the probability of sample i belonging to class j\n",
    "Log Loss has no upper bound and it exists on the range [0, ∞). Log Loss nearer to 0 indicates higher accuracy, whereas if the Log Loss is away from 0 then it indicates lower accuracy.\n",
    "In general, minimising Log Loss gives greater accuracy for the classifier.\n",
    "\n",
    "# Confusion Matrix\n",
    "Confusion Matrix as the name suggests gives us a matrix as output and describes the complete performance of the model.\n",
    "Lets assume we have a binary classification problem. We have some samples belonging to two classes : YES or NO. Also, we have our own classifier which predicts a class for a given input sample. On testing our model on 165 samples ,we get the following result.\n",
    "![](https://miro.medium.com/max/386/1*GMlSubndVt3g7FmeQjpeMA.png)\n",
    "\n",
    "There are 4 important terms :\n",
    "True Positives : The cases in which we predicted YES and the actual output was also YES.\n",
    "True Negatives : The cases in which we predicted NO and the actual output was NO.\n",
    "False Positives : The cases in which we predicted YES and the actual output was NO.\n",
    "False Negatives : The cases in which we predicted NO and the actual output was YES.\n",
    "Accuracy for the matrix can be calculated by taking average of the values lying across the “main diagonal” i.e.\n",
    "![](https://miro.medium.com/max/366/1*NDD6vdbTcLPzXVbCKi87RA.gif)\n",
    "![](https://miro.medium.com/max/241/1*VbjXSHvKxtgoi-oM2OHuuQ.gif)\n",
    "\n",
    "Confusion Matrix forms the basis for the other types of metrics.\n",
    "\n",
    "# Area Under Curve\n",
    "Area Under Curve(AUC) is one of the most widely used metrics for evaluation. It is used for binary classification problem. AUC of a classifier is equal to the probability that the classifier will rank a randomly chosen positive example higher than a randomly chosen negative example. Before defining AUC, let us understand two basic terms :\n",
    "\n",
    "**True Positive Rate (Sensitivity)** : True Positive Rate is defined as TP/ (FN+TP). True Positive Rate corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points.\n",
    "![](https://miro.medium.com/max/420/1*yw4Y3D7nGNVza2EC2WrOfg.gif)\n",
    "\n",
    "**False Positive Rate (Specificity)** : False Positive Rate is defined as FP / (FP+TN). False Positive Rate corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points.\n",
    "![](https://miro.medium.com/max/426/1*eIIZS8FNvpxiEzHpFI3QZA.gif)\n",
    "\n",
    "False Positive Rate and True Positive Rate both have values in the range [0, 1]. FPR and TPR bot hare computed at threshold values such as (0.00, 0.02, 0.04, …., 1.00) and a graph is drawn. AUC is the area under the curve of plot False Positive Rate vs True Positive Rate at different points in [0, 1].\n",
    "![](https://miro.medium.com/max/640/1*zFW1Kj3e2X_mmluTW3rVeA.png)\n",
    "As evident, AUC has a range of [0, 1]. The greater the value, the better is the performance of our model.\n",
    "\n",
    "# F1 Score\n",
    "F1 Score is used to measure a test’s accuracy.F1 Score is the Harmonic Mean between precision and recall. The range for F1 Score is [0, 1]. It tells you how precise your classifier is (how many instances it classifies correctly), as well as how robust it is (it does not miss a significant number of instances).\n",
    "High precision but lower recall, gives you an extremely accurate, but it then misses a large number of instances that are difficult to classify. The greater the F1 Score, the better is the performance of our model. Mathematically, it can be expressed as :\n",
    "![](https://miro.medium.com/max/191/1*_pYttqYh8w-EpLxMi84H8A.gif)\n",
    "\n",
    "F1 Score tries to find the balance between precision and recall.\n",
    "\n",
    "**Precision** : It is the number of correct positive results divided by the number of positive results predicted by the classifier.\n",
    "![](https://miro.medium.com/max/365/1*wMIDfGwT9bA6HezvYhdbpQ.gif)\n",
    "\n",
    "\n",
    "**Recall** : It is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive).\n",
    "![](https://miro.medium.com/max/371/1*gIlQMZBPjtUHWkwOhRG3KA.gif)\n",
    "\n",
    "# Mean Absolute Error\n",
    "Mean Absolute Error is the average of the difference between the Original Values and the Predicted Values. It gives us the measure of how far the predictions were from the actual output. However, they don’t gives us any idea of the direction of the error i.e. whether we are under predicting the data or over predicting the data. Mathematically, it is represented as : ![](https://miro.medium.com/max/303/1*qak4Dadzs7pO0hnz4q8O8Q.gif)\n",
    "\n",
    "# Mean Squared Error\n",
    "Mean Squared Error(MSE) is quite similar to Mean Absolute Error, the only difference being that MSE takes the average of the square of the difference between the original values and the predicted values. The advantage of MSE being that it is easier to compute the gradient, whereas Mean Absolute Error requires complicated linear programming tools to compute the gradient. As, we take square of the error, the effect of larger errors become more pronounced then smaller error, hence the model can now focus more on the larger errors.\n",
    "![](https://miro.medium.com/max/312/1*okvAVQNY6s5cMHxrqUzM5A.gif)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQVJh4IF1nDm"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score #importing confusion_matrix and accuracy_score from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "u3aplE701xqx",
    "outputId": "6029621c-0937-4fe5-95fd-f3dc96e1a7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 41 121 355 106]\n",
      " [ 53 110 342 115]\n",
      " [ 48 127 336 109]\n",
      " [ 50 111 370  93]]\n"
     ]
    }
   ],
   "source": [
    "cm=confusion_matrix(y_true,y_pred) #confusion matrix takes 2 arguments i.e. actual and predicted values\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ytGP2i2j2Qk3",
    "outputId": "c373e24b-28f9-402d-bfc4-799b40a3b925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23321270607157218\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_true,y_pred)) #accuracy_score takes 2 arguments i.e. actual and predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OBoL4-12oLW"
   },
   "source": [
    "# SUMMARY \n",
    "\n",
    "Our aim was to develop a deep learning model to identify four different types of White Blood cells found in humans that are Eosinophils , Monocytes , Lymphocytes and Neutrophils. Primary approach towards this problem was to develop a Convolutional Neural Network model that can classify blood smear slides into four categories mentioned above.Deep learning framework which we used for this problem was Keras.We used ImageDataGenerator class of Keras to load images directly from directory and for Image Preprocessing purposes like Rescaling.OpenCV was used to display the images and have a better intuition about the problem.A 14 layer deep CNN was implemented in which we used Dropout layers for regularization purpose to prevent overfitting.as being a multiclass classification problem we used softmax activation function in the last Dense layer followed by categorical_crossentropy as loss function and accuracy as classification metrics.After training of CNN model we managed to get a very good training and validation accuracy of 92.66% and 90.19% respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
